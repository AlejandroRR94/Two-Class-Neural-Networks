{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll use a simple Neural Network in TensorFlow to classify the clients of a bank as two classes:\n",
    "\n",
    "- Will open a deposit\n",
    "- Will not open a deposit\n",
    "\n",
    "\n",
    "**SPOILER:** The dataset is imbalanced, as it is the same dataset I've used to make a notebook addressing this problem. However, what counts is that the Neural Network is implemented and the results make sense (taking into account the imbalance in the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the content up until the neural network will be incredibly similar to the imbalanced data, since I'm downloading the data and converting the categorical data into numerical, one-hot encoding the columns that had multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data and data wrangling\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "#Linear Algebra\n",
    "import numpy as np\n",
    "\n",
    "#Statistics\n",
    "import scipy as sp\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading dataset from openml\n",
    "data = datasets.fetch_openml(\n",
    "    'bank-marketing',\n",
    "    version = 'active',\n",
    "    return_X_y = True,\n",
    "    as_frame = True\n",
    ")\n",
    "\n",
    "X, y = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I provide the actual names of the columns to build a more comprehensive\n",
    "# dataset and to know what we are looking at.\n",
    "new_variables = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan','contact','day','month','duration','campaign','pdays', 'previous',\n",
    "       'poutcome', 'y']\n",
    "\n",
    "variables_dict={}\n",
    "for index,old in enumerate(list(X.columns)):\n",
    "    variables_dict[old] = new_variables[index]\n",
    "    \n",
    "#We rename the dataframe's columns\n",
    "X = X.rename(columns = variables_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating y and concatenating both X and y to form a dataframe\n",
    "y = y.to_frame('y')\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y = y.rename(columns = {'Class':'y'})\n",
    "\n",
    "df = pd.concat([X,y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = enc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jul</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  balance  day  duration  campaign  pdays  previous  job_blue-collar  \\\n",
       "0  58.0   2143.0  5.0     261.0       1.0   -1.0       0.0                0   \n",
       "1  44.0     29.0  5.0     151.0       1.0   -1.0       0.0                0   \n",
       "2  33.0      2.0  5.0      76.0       1.0   -1.0       0.0                0   \n",
       "3  47.0   1506.0  5.0      92.0       1.0   -1.0       0.0                1   \n",
       "4  33.0      1.0  5.0     198.0       1.0   -1.0       0.0                0   \n",
       "\n",
       "   job_entrepreneur  job_housemaid  ...  month_jul  month_jun  month_mar  \\\n",
       "0                 0              0  ...          0          0          0   \n",
       "1                 0              0  ...          0          0          0   \n",
       "2                 1              0  ...          0          0          0   \n",
       "3                 0              0  ...          0          0          0   \n",
       "4                 0              0  ...          0          0          0   \n",
       "\n",
       "   month_may  month_nov  month_oct  month_sep  poutcome_other  \\\n",
       "0          1          0          0          0               0   \n",
       "1          1          0          0          0               0   \n",
       "2          1          0          0          0               0   \n",
       "3          1          0          0          0               0   \n",
       "4          1          0          0          0               0   \n",
       "\n",
       "   poutcome_success  poutcome_unknown  \n",
       "0                 0                 1  \n",
       "1                 0                 1  \n",
       "2                 0                 1  \n",
       "3                 0                 1  \n",
       "4                 0                 1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(X, drop_first = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_category = [df.columns.get_loc(c) for c in list(df.select_dtypes(include='category').columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_transform = list(df.select_dtypes(include='category').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'poutcome']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_to_transform = categories_to_transform[:-1]\n",
    "categories_to_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns = list(df.select_dtypes(include='float64').columns)\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OHE = pd.concat([df[numerical_columns],\n",
    "                    pd.get_dummies(df[categories_to_transform],\n",
    "                                  drop_first = True), y],\n",
    "                    axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  balance  day  duration  campaign  pdays  previous  job_blue-collar  \\\n",
       "0  58.0   2143.0  5.0     261.0       1.0   -1.0       0.0                0   \n",
       "1  44.0     29.0  5.0     151.0       1.0   -1.0       0.0                0   \n",
       "2  33.0      2.0  5.0      76.0       1.0   -1.0       0.0                0   \n",
       "3  47.0   1506.0  5.0      92.0       1.0   -1.0       0.0                1   \n",
       "4  33.0      1.0  5.0     198.0       1.0   -1.0       0.0                0   \n",
       "\n",
       "   job_entrepreneur  job_housemaid  ...  month_jun  month_mar  month_may  \\\n",
       "0                 0              0  ...          0          0          1   \n",
       "1                 0              0  ...          0          0          1   \n",
       "2                 1              0  ...          0          0          1   \n",
       "3                 0              0  ...          0          0          1   \n",
       "4                 0              0  ...          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_other  poutcome_success  \\\n",
       "0          0          0          0               0                 0   \n",
       "1          0          0          0               0                 0   \n",
       "2          0          0          0               0                 0   \n",
       "3          0          0          0               0                 0   \n",
       "4          0          0          0               0                 0   \n",
       "\n",
       "   poutcome_unknown  y  \n",
       "0                 1  1  \n",
       "1                 1  1  \n",
       "2                 1  1  \n",
       "3                 1  1  \n",
       "4                 1  1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_OHE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {1 : 0, 2 : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OHE['y'] = data_OHE['y'].replace(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_OHE.drop('y', axis = 1), \n",
    "                                                    data_OHE['y'],\n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first. To build the Neural Network I'll use Keras&TensorFlow 2.0.\n",
    "\n",
    "With Keras the Neural Networks are defined as a series of layers built **sequentially** upon each other, hence the **Sequential()** function that is used to initialize the Neural Network.\n",
    "\n",
    "It will pretty much go like this:\n",
    "\n",
    "- Initialize the Neural Network\n",
    "\n",
    "\n",
    "- Place as many layers of neurons as you like, specifying:\n",
    "\n",
    "    - Number of Neurons (parameter **units**)\n",
    "    - How the initial weights of the neurons will be set (parameter **kernel_initializer**)\n",
    "    - The activation Function (parameter **activation**)\n",
    "    - The input dimensions, how many attributes will the neuron receive.\n",
    "    \n",
    "    \n",
    "- Define the output layer with one single neuron and an activation function that fits the purpose (sigmoid and softmax being the most used)\n",
    "\n",
    "\n",
    "- Tell the neural network how it has to learn to predict with the **compile** method:\n",
    "    - Optimizer\n",
    "    - Loss\n",
    "    - Metric\n",
    "    \n",
    "From here on it will be the same as has been done for simpler machine learning algorithms:\n",
    "\n",
    "\n",
    "- Fit the Neural Network to the training set\n",
    "\n",
    "- Predict the test values\n",
    "\n",
    "- Evaluate performance and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31647, 42)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network, take 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Neural Network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First layer of Neurons\n",
    "model.add(Dense(6,\n",
    "               kernel_initializer = \"uniform\",\n",
    "               activation = \"relu\",\n",
    "                input_dim = X_train.shape[1] #Segundo atributo del método\n",
    "                #shape, las columnas\n",
    "               ))\n",
    "#Second layer of Neurons\n",
    "model.add(Dense(units = 6,\n",
    "               kernel_initializer = \"uniform\",\n",
    "               activation = \"relu\",\n",
    "               ))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(units = 1,\n",
    "               kernel_initializer = \"uniform\",\n",
    "               activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining how the Neural Network will learn\n",
    "model.compile(optimizer = SGD(learning_rate = 0.05),\n",
    "             loss = \"binary_crossentropy\",\n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31647 samples\n",
      "Epoch 1/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.3587 - accuracy: 0.8837\n",
      "Epoch 2/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2309 - accuracy: 0.9033\n",
      "Epoch 3/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2208 - accuracy: 0.9029\n",
      "Epoch 4/50\n",
      "31647/31647 [==============================] - 1s 41us/sample - loss: 0.2164 - accuracy: 0.9038\n",
      "Epoch 5/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2139 - accuracy: 0.9032\n",
      "Epoch 6/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2121 - accuracy: 0.9050\n",
      "Epoch 7/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2103 - accuracy: 0.9050\n",
      "Epoch 8/50\n",
      "31647/31647 [==============================] - 1s 42us/sample - loss: 0.2090 - accuracy: 0.9058\n",
      "Epoch 9/50\n",
      "31647/31647 [==============================] - 2s 49us/sample - loss: 0.2077 - accuracy: 0.9071\n",
      "Epoch 10/50\n",
      "31647/31647 [==============================] - 1s 45us/sample - loss: 0.2067 - accuracy: 0.9074\n",
      "Epoch 11/50\n",
      "31647/31647 [==============================] - 1s 46us/sample - loss: 0.2059 - accuracy: 0.9070\n",
      "Epoch 12/50\n",
      "31647/31647 [==============================] - 2s 48us/sample - loss: 0.2048 - accuracy: 0.9068\n",
      "Epoch 13/50\n",
      "31647/31647 [==============================] - 1s 45us/sample - loss: 0.2041 - accuracy: 0.9081\n",
      "Epoch 14/50\n",
      "31647/31647 [==============================] - 1s 44us/sample - loss: 0.2036 - accuracy: 0.9073\n",
      "Epoch 15/50\n",
      "31647/31647 [==============================] - 1s 44us/sample - loss: 0.2031 - accuracy: 0.9083\n",
      "Epoch 16/50\n",
      "31647/31647 [==============================] - 1s 45us/sample - loss: 0.2020 - accuracy: 0.9090\n",
      "Epoch 17/50\n",
      "31647/31647 [==============================] - 1s 45us/sample - loss: 0.2017 - accuracy: 0.9082\n",
      "Epoch 18/50\n",
      "31647/31647 [==============================] - 1s 46us/sample - loss: 0.2011 - accuracy: 0.9085\n",
      "Epoch 19/50\n",
      "31647/31647 [==============================] - 2s 48us/sample - loss: 0.2004 - accuracy: 0.9092\n",
      "Epoch 20/50\n",
      "31647/31647 [==============================] - 2s 59us/sample - loss: 0.2002 - accuracy: 0.9097\n",
      "Epoch 21/50\n",
      "31647/31647 [==============================] - 2s 53us/sample - loss: 0.1995 - accuracy: 0.9104\n",
      "Epoch 22/50\n",
      "31647/31647 [==============================] - 2s 49us/sample - loss: 0.1995 - accuracy: 0.9095\n",
      "Epoch 23/50\n",
      "31647/31647 [==============================] - 2s 55us/sample - loss: 0.1988 - accuracy: 0.9097\n",
      "Epoch 24/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1988 - accuracy: 0.9101\n",
      "Epoch 25/50\n",
      "31647/31647 [==============================] - 2s 52us/sample - loss: 0.1981 - accuracy: 0.9109\n",
      "Epoch 26/50\n",
      "31647/31647 [==============================] - 2s 53us/sample - loss: 0.1983 - accuracy: 0.9106\n",
      "Epoch 27/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1979 - accuracy: 0.9096\n",
      "Epoch 28/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1973 - accuracy: 0.9111\n",
      "Epoch 29/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1972 - accuracy: 0.9121\n",
      "Epoch 30/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1966 - accuracy: 0.9115\n",
      "Epoch 31/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1969 - accuracy: 0.9105\n",
      "Epoch 32/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1960 - accuracy: 0.9110\n",
      "Epoch 33/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1959 - accuracy: 0.9113\n",
      "Epoch 34/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1962 - accuracy: 0.9120\n",
      "Epoch 35/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1958 - accuracy: 0.9133\n",
      "Epoch 36/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1956 - accuracy: 0.9125\n",
      "Epoch 37/50\n",
      "31647/31647 [==============================] - 2s 53us/sample - loss: 0.1952 - accuracy: 0.9124\n",
      "Epoch 38/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1952 - accuracy: 0.9124\n",
      "Epoch 39/50\n",
      "31647/31647 [==============================] - 2s 53us/sample - loss: 0.1950 - accuracy: 0.9136\n",
      "Epoch 40/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1948 - accuracy: 0.9128\n",
      "Epoch 41/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1949 - accuracy: 0.9121\n",
      "Epoch 42/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1945 - accuracy: 0.9129\n",
      "Epoch 43/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1946 - accuracy: 0.9129\n",
      "Epoch 44/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1941 - accuracy: 0.9136\n",
      "Epoch 45/50\n",
      "31647/31647 [==============================] - 2s 51us/sample - loss: 0.1939 - accuracy: 0.9127\n",
      "Epoch 46/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1937 - accuracy: 0.9127\n",
      "Epoch 47/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1939 - accuracy: 0.9131\n",
      "Epoch 48/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1937 - accuracy: 0.9130\n",
      "Epoch 49/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1932 - accuracy: 0.9128\n",
      "Epoch 50/50\n",
      "31647/31647 [==============================] - 2s 50us/sample - loss: 0.1934 - accuracy: 0.9137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dba91aedc8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 20, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Neural Network returns probabilities, lets make use of a threshold to classify the probabilities as 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11476,   493],\n",
       "       [  809,   786]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting a fancier confusion matrix\n",
    "fig, ax = plt.subplots(figsize = (2.5, 2.5))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x = j, y = i,\n",
    "               s = cm[i,j])\n",
    "        \n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the fact that the neural network got more than 90% accuracy quickly, there is not much to say. The results make perfect sense, since we knew from the beginning that our dataset was imbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network or Machine Learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network outperformed models that implemented a Logistic Regression or a Random Forest. For the question \"is it worth it to implement a Neural Network?\" to be asked, we should put the NN to the test with the bias problem solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the previous neural network in a function and use it after solving the balance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Network():\n",
    "    #Defining the Neural Network\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(6,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"relu\",\n",
    "                    input_dim = X_train.shape[1] #Segundo atributo del método\n",
    "                    #shape, las columnas\n",
    "                   ))\n",
    "\n",
    "    model.add(Dense(units = 6,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"relu\",\n",
    "                   ))\n",
    "\n",
    "    model.add(Dense(units = 1,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"sigmoid\"))\n",
    "    \n",
    "        #Definimos cómo realizará la red neuronal el aprendizaje\n",
    "    model.compile(optimizer = RMSprop(learning_rate = 0.01),\n",
    "                 loss = \"binary_crossentropy\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling the minority class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook that addresses the imbalanced data, the best solution was to upsample the minority class, reason why I implement that very same solution here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign \n",
    "minority = data_OHE[data_OHE['y']==1]\n",
    "majority = data_OHE[data_OHE['y']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    39922\n",
       "0    39922\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase the number of minority samples\n",
    "minority_up = resample(minority,\n",
    "                        replace = True,\n",
    "                        n_samples = len(majority),\n",
    "                        random_state = 27)\n",
    "\n",
    "upsampled = pd.concat([majority, minority_up])\n",
    "\n",
    "#Check the number of samples\n",
    "upsampled.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is important to assign the training and testing sets after generating the data with the upsampled/downsized\n",
    "# minority/majority\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(upsampled.drop('y', axis = 1), \n",
    "                                                    upsampled['y'],\n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "#DON'T FORGET TO SCALE THE TRAINING SETS AGAIN\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network, take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the Neural_Network function to the neural variable\n",
    "neural = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55890 samples\n",
      "Epoch 1/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4261 - accuracy: 0.8286\n",
      "Epoch 2/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3644 - accuracy: 0.8466\n",
      "Epoch 3/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3547 - accuracy: 0.8505\n",
      "Epoch 4/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3501 - accuracy: 0.8522\n",
      "Epoch 5/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3471 - accuracy: 0.8527\n",
      "Epoch 6/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3460 - accuracy: 0.8533\n",
      "Epoch 7/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3442 - accuracy: 0.8559\n",
      "Epoch 8/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3431 - accuracy: 0.8560\n",
      "Epoch 9/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3420 - accuracy: 0.8565\n",
      "Epoch 10/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3409 - accuracy: 0.8570\n",
      "Epoch 11/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3400 - accuracy: 0.8580\n",
      "Epoch 12/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3395 - accuracy: 0.8578\n",
      "Epoch 13/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3388 - accuracy: 0.8570\n",
      "Epoch 14/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3383 - accuracy: 0.8580\n",
      "Epoch 15/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3378 - accuracy: 0.8593\n",
      "Epoch 16/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3376 - accuracy: 0.8589\n",
      "Epoch 17/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3372 - accuracy: 0.8593\n",
      "Epoch 18/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3369 - accuracy: 0.8587\n",
      "Epoch 19/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3364 - accuracy: 0.8587\n",
      "Epoch 20/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3366 - accuracy: 0.8604\n",
      "Epoch 21/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3362 - accuracy: 0.8585\n",
      "Epoch 22/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3363 - accuracy: 0.8591\n",
      "Epoch 23/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3356 - accuracy: 0.8595\n",
      "Epoch 24/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3360 - accuracy: 0.8590\n",
      "Epoch 25/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3356 - accuracy: 0.8599\n",
      "Epoch 26/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3363 - accuracy: 0.8602\n",
      "Epoch 27/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3357 - accuracy: 0.8606\n",
      "Epoch 28/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3353 - accuracy: 0.8610\n",
      "Epoch 29/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3359 - accuracy: 0.8599\n",
      "Epoch 30/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3356 - accuracy: 0.8595\n",
      "Epoch 31/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3350 - accuracy: 0.8605\n",
      "Epoch 32/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3354 - accuracy: 0.8607\n",
      "Epoch 33/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3349 - accuracy: 0.8609\n",
      "Epoch 34/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3348 - accuracy: 0.8609\n",
      "Epoch 35/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3348 - accuracy: 0.8608\n",
      "Epoch 36/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3344 - accuracy: 0.8609\n",
      "Epoch 37/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3339 - accuracy: 0.8608\n",
      "Epoch 38/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3335 - accuracy: 0.8617\n",
      "Epoch 39/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3324 - accuracy: 0.8608\n",
      "Epoch 40/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 41/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3319 - accuracy: 0.8623\n",
      "Epoch 42/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3317 - accuracy: 0.8620\n",
      "Epoch 43/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3310 - accuracy: 0.8623\n",
      "Epoch 44/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3302 - accuracy: 0.8627\n",
      "Epoch 45/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3306 - accuracy: 0.8614\n",
      "Epoch 46/300\n",
      "55890/55890 [==============================] - 0s 6us/sample - loss: 0.3308 - accuracy: 0.8621\n",
      "Epoch 47/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3305 - accuracy: 0.8620\n",
      "Epoch 48/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3302 - accuracy: 0.8619\n",
      "Epoch 49/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3303 - accuracy: 0.8635\n",
      "Epoch 50/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3301 - accuracy: 0.8619\n",
      "Epoch 51/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3301 - accuracy: 0.8623\n",
      "Epoch 52/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3299 - accuracy: 0.8610\n",
      "Epoch 53/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3305 - accuracy: 0.8624\n",
      "Epoch 54/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3301 - accuracy: 0.8613\n",
      "Epoch 55/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3304 - accuracy: 0.8617\n",
      "Epoch 56/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.3299 - accuracy: 0.8611\n",
      "Epoch 57/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3298 - accuracy: 0.8612\n",
      "Epoch 58/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3301 - accuracy: 0.8609\n",
      "Epoch 59/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3306 - accuracy: 0.8621\n",
      "Epoch 60/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3304 - accuracy: 0.8605\n",
      "Epoch 61/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3303 - accuracy: 0.8621\n",
      "Epoch 62/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3306 - accuracy: 0.8620\n",
      "Epoch 63/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3311 - accuracy: 0.8614\n",
      "Epoch 64/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3301 - accuracy: 0.8616\n",
      "Epoch 65/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3290 - accuracy: 0.8633\n",
      "Epoch 66/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3288 - accuracy: 0.8638\n",
      "Epoch 67/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 68/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3277 - accuracy: 0.8635\n",
      "Epoch 69/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.3270 - accuracy: 0.8637\n",
      "Epoch 70/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3276 - accuracy: 0.8636\n",
      "Epoch 71/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3271 - accuracy: 0.8627\n",
      "Epoch 72/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3269 - accuracy: 0.8636\n",
      "Epoch 73/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3266 - accuracy: 0.8636\n",
      "Epoch 74/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3257 - accuracy: 0.8641\n",
      "Epoch 75/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3260 - accuracy: 0.8636\n",
      "Epoch 76/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3250 - accuracy: 0.8642\n",
      "Epoch 77/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3254 - accuracy: 0.8638\n",
      "Epoch 78/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3258 - accuracy: 0.8629\n",
      "Epoch 79/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3248 - accuracy: 0.8632\n",
      "Epoch 80/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.3247 - accuracy: 0.8637\n",
      "Epoch 81/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.3246 - accuracy: 0.8640\n",
      "Epoch 82/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3245 - accuracy: 0.8638\n",
      "Epoch 83/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3240 - accuracy: 0.8634\n",
      "Epoch 84/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3238 - accuracy: 0.8648\n",
      "Epoch 85/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3239 - accuracy: 0.8636\n",
      "Epoch 86/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3239 - accuracy: 0.8642\n",
      "Epoch 87/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3235 - accuracy: 0.8637\n",
      "Epoch 88/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3238 - accuracy: 0.8648\n",
      "Epoch 89/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3236 - accuracy: 0.8647\n",
      "Epoch 90/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3231 - accuracy: 0.8652\n",
      "Epoch 91/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3232 - accuracy: 0.8643\n",
      "Epoch 92/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3234 - accuracy: 0.8646\n",
      "Epoch 93/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3228 - accuracy: 0.8642\n",
      "Epoch 94/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3233 - accuracy: 0.8649\n",
      "Epoch 95/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3227 - accuracy: 0.8651\n",
      "Epoch 96/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3232 - accuracy: 0.8646\n",
      "Epoch 97/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3228 - accuracy: 0.8661\n",
      "Epoch 98/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3232 - accuracy: 0.8647\n",
      "Epoch 99/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3233 - accuracy: 0.8649\n",
      "Epoch 100/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3231 - accuracy: 0.8649\n",
      "Epoch 101/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3231 - accuracy: 0.8648\n",
      "Epoch 102/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.3227 - accuracy: 0.8654\n",
      "Epoch 103/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3223 - accuracy: 0.8661\n",
      "Epoch 104/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3227 - accuracy: 0.8657\n",
      "Epoch 105/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3223 - accuracy: 0.8650\n",
      "Epoch 106/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3228 - accuracy: 0.8656\n",
      "Epoch 107/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3224 - accuracy: 0.8652\n",
      "Epoch 108/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3227 - accuracy: 0.8658\n",
      "Epoch 109/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3220 - accuracy: 0.8665\n",
      "Epoch 110/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3226 - accuracy: 0.8651\n",
      "Epoch 111/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3222 - accuracy: 0.8655\n",
      "Epoch 112/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3220 - accuracy: 0.8659\n",
      "Epoch 113/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3224 - accuracy: 0.8653\n",
      "Epoch 114/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3224 - accuracy: 0.8658\n",
      "Epoch 115/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3221 - accuracy: 0.8654\n",
      "Epoch 116/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3226 - accuracy: 0.8655\n",
      "Epoch 117/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3221 - accuracy: 0.8655\n",
      "Epoch 118/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3220 - accuracy: 0.8651\n",
      "Epoch 119/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3223 - accuracy: 0.8666\n",
      "Epoch 120/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3217 - accuracy: 0.8669\n",
      "Epoch 121/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3221 - accuracy: 0.8656\n",
      "Epoch 122/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3218 - accuracy: 0.8667\n",
      "Epoch 123/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3217 - accuracy: 0.8656\n",
      "Epoch 124/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3216 - accuracy: 0.8654\n",
      "Epoch 125/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3217 - accuracy: 0.8666\n",
      "Epoch 126/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3215 - accuracy: 0.8665\n",
      "Epoch 127/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3214 - accuracy: 0.8660\n",
      "Epoch 128/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3216 - accuracy: 0.8667\n",
      "Epoch 129/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3214 - accuracy: 0.8672\n",
      "Epoch 130/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3213 - accuracy: 0.8664\n",
      "Epoch 131/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8665\n",
      "Epoch 132/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3212 - accuracy: 0.8675\n",
      "Epoch 133/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3218 - accuracy: 0.8655\n",
      "Epoch 134/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8655\n",
      "Epoch 135/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3215 - accuracy: 0.8668\n",
      "Epoch 136/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3215 - accuracy: 0.8673\n",
      "Epoch 137/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3215 - accuracy: 0.8657\n",
      "Epoch 138/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3214 - accuracy: 0.8672\n",
      "Epoch 139/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8668\n",
      "Epoch 140/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8664\n",
      "Epoch 141/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3214 - accuracy: 0.8663\n",
      "Epoch 142/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3212 - accuracy: 0.8668\n",
      "Epoch 143/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3212 - accuracy: 0.8660\n",
      "Epoch 144/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3208 - accuracy: 0.8667\n",
      "Epoch 145/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8663\n",
      "Epoch 146/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8669\n",
      "Epoch 147/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8664\n",
      "Epoch 148/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3201 - accuracy: 0.8656\n",
      "Epoch 149/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3210 - accuracy: 0.8659\n",
      "Epoch 150/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3212 - accuracy: 0.8667\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8677\n",
      "Epoch 152/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3210 - accuracy: 0.8658\n",
      "Epoch 153/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3211 - accuracy: 0.8668\n",
      "Epoch 154/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3208 - accuracy: 0.8673\n",
      "Epoch 155/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3204 - accuracy: 0.8671\n",
      "Epoch 156/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8665\n",
      "Epoch 157/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3209 - accuracy: 0.8665\n",
      "Epoch 158/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3211 - accuracy: 0.8665\n",
      "Epoch 159/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3206 - accuracy: 0.8669\n",
      "Epoch 160/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3207 - accuracy: 0.8663\n",
      "Epoch 161/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3206 - accuracy: 0.8670\n",
      "Epoch 162/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3203 - accuracy: 0.8677\n",
      "Epoch 163/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3206 - accuracy: 0.8665\n",
      "Epoch 164/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3203 - accuracy: 0.8664\n",
      "Epoch 165/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3202 - accuracy: 0.8667\n",
      "Epoch 166/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3200 - accuracy: 0.8666\n",
      "Epoch 167/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3201 - accuracy: 0.8662\n",
      "Epoch 168/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3203 - accuracy: 0.8670\n",
      "Epoch 169/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8670\n",
      "Epoch 170/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3203 - accuracy: 0.8674\n",
      "Epoch 171/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3204 - accuracy: 0.8659\n",
      "Epoch 172/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3198 - accuracy: 0.8678\n",
      "Epoch 173/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8669\n",
      "Epoch 174/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3199 - accuracy: 0.8678\n",
      "Epoch 175/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8678\n",
      "Epoch 176/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3196 - accuracy: 0.8671\n",
      "Epoch 177/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3200 - accuracy: 0.8670\n",
      "Epoch 178/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3200 - accuracy: 0.8659\n",
      "Epoch 179/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3196 - accuracy: 0.8675\n",
      "Epoch 180/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3200 - accuracy: 0.8664\n",
      "Epoch 181/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3190 - accuracy: 0.8684\n",
      "Epoch 182/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3198 - accuracy: 0.8666\n",
      "Epoch 183/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8673\n",
      "Epoch 184/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3192 - accuracy: 0.8673\n",
      "Epoch 185/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3202 - accuracy: 0.8663\n",
      "Epoch 186/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8673\n",
      "Epoch 187/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3198 - accuracy: 0.8664\n",
      "Epoch 188/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3196 - accuracy: 0.8666\n",
      "Epoch 189/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8678\n",
      "Epoch 190/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8673\n",
      "Epoch 191/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3193 - accuracy: 0.8670\n",
      "Epoch 192/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3193 - accuracy: 0.8672\n",
      "Epoch 193/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8671\n",
      "Epoch 194/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8677\n",
      "Epoch 195/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3197 - accuracy: 0.8664\n",
      "Epoch 196/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3194 - accuracy: 0.8665\n",
      "Epoch 197/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8671\n",
      "Epoch 198/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8673\n",
      "Epoch 199/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3193 - accuracy: 0.8674\n",
      "Epoch 200/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8660\n",
      "Epoch 201/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3198 - accuracy: 0.8667\n",
      "Epoch 202/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3194 - accuracy: 0.8680\n",
      "Epoch 203/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8673\n",
      "Epoch 204/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8677\n",
      "Epoch 205/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3188 - accuracy: 0.8679\n",
      "Epoch 206/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3189 - accuracy: 0.8674\n",
      "Epoch 207/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8677\n",
      "Epoch 208/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3198 - accuracy: 0.8669\n",
      "Epoch 209/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8676\n",
      "Epoch 210/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3192 - accuracy: 0.8680\n",
      "Epoch 211/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3192 - accuracy: 0.8673\n",
      "Epoch 212/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3196 - accuracy: 0.8671\n",
      "Epoch 213/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8678\n",
      "Epoch 214/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3190 - accuracy: 0.8678\n",
      "Epoch 215/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3190 - accuracy: 0.8678\n",
      "Epoch 216/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3185 - accuracy: 0.8682\n",
      "Epoch 217/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 218/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3189 - accuracy: 0.8682\n",
      "Epoch 219/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3187 - accuracy: 0.8670\n",
      "Epoch 220/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3195 - accuracy: 0.8668\n",
      "Epoch 221/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3189 - accuracy: 0.8677\n",
      "Epoch 222/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3188 - accuracy: 0.8675\n",
      "Epoch 223/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3190 - accuracy: 0.8657\n",
      "Epoch 224/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3186 - accuracy: 0.8670\n",
      "Epoch 225/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3187 - accuracy: 0.8676\n",
      "Epoch 226/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3191 - accuracy: 0.8671\n",
      "Epoch 227/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3185 - accuracy: 0.8669\n",
      "Epoch 228/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3190 - accuracy: 0.8684\n",
      "Epoch 229/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 230/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3187 - accuracy: 0.8673\n",
      "Epoch 231/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3188 - accuracy: 0.8678\n",
      "Epoch 232/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3180 - accuracy: 0.8682\n",
      "Epoch 233/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3175 - accuracy: 0.8678\n",
      "Epoch 234/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3179 - accuracy: 0.8685\n",
      "Epoch 235/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3177 - accuracy: 0.8682\n",
      "Epoch 236/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3180 - accuracy: 0.8680\n",
      "Epoch 237/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3178 - accuracy: 0.8678\n",
      "Epoch 238/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.3178 - accuracy: 0.8685\n",
      "Epoch 239/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.3181 - accuracy: 0.8679\n",
      "Epoch 240/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3175 - accuracy: 0.8679\n",
      "Epoch 241/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.3178 - accuracy: 0.8678\n",
      "Epoch 242/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.3176 - accuracy: 0.8682\n",
      "Epoch 243/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3173 - accuracy: 0.8682\n",
      "Epoch 244/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3179 - accuracy: 0.8682\n",
      "Epoch 245/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8684\n",
      "Epoch 246/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3170 - accuracy: 0.8684\n",
      "Epoch 247/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3173 - accuracy: 0.8682\n",
      "Epoch 248/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8674\n",
      "Epoch 249/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3171 - accuracy: 0.8673\n",
      "Epoch 250/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3173 - accuracy: 0.8677\n",
      "Epoch 251/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3170 - accuracy: 0.8688\n",
      "Epoch 252/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8689\n",
      "Epoch 253/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3171 - accuracy: 0.8680\n",
      "Epoch 254/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3172 - accuracy: 0.8681\n",
      "Epoch 255/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8685\n",
      "Epoch 256/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3171 - accuracy: 0.8671\n",
      "Epoch 257/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8683\n",
      "Epoch 258/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8685\n",
      "Epoch 259/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8676\n",
      "Epoch 260/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8691\n",
      "Epoch 261/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8688\n",
      "Epoch 262/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8685\n",
      "Epoch 263/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8684\n",
      "Epoch 264/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8687\n",
      "Epoch 265/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8682\n",
      "Epoch 266/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8686\n",
      "Epoch 267/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3161 - accuracy: 0.8687\n",
      "Epoch 268/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8691\n",
      "Epoch 269/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3169 - accuracy: 0.8692\n",
      "Epoch 270/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8690\n",
      "Epoch 271/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3167 - accuracy: 0.8686\n",
      "Epoch 272/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8694\n",
      "Epoch 273/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3166 - accuracy: 0.8691\n",
      "Epoch 274/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8681\n",
      "Epoch 275/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8682\n",
      "Epoch 276/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3166 - accuracy: 0.8687\n",
      "Epoch 277/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8682\n",
      "Epoch 278/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8685\n",
      "Epoch 279/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8693\n",
      "Epoch 280/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8682\n",
      "Epoch 281/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8689\n",
      "Epoch 282/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8694\n",
      "Epoch 283/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3168 - accuracy: 0.8687\n",
      "Epoch 284/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8698\n",
      "Epoch 285/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8696\n",
      "Epoch 286/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3160 - accuracy: 0.8692\n",
      "Epoch 287/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3161 - accuracy: 0.8696\n",
      "Epoch 288/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3161 - accuracy: 0.8694\n",
      "Epoch 289/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3160 - accuracy: 0.8694\n",
      "Epoch 290/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3165 - accuracy: 0.8689\n",
      "Epoch 291/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8682\n",
      "Epoch 292/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8688\n",
      "Epoch 293/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8688\n",
      "Epoch 294/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8698\n",
      "Epoch 295/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8683\n",
      "Epoch 296/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3162 - accuracy: 0.8695\n",
      "Epoch 297/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8682\n",
      "Epoch 298/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3164 - accuracy: 0.8684\n",
      "Epoch 299/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3161 - accuracy: 0.8688\n",
      "Epoch 300/300\n",
      "55890/55890 [==============================] - 0s 7us/sample - loss: 0.3163 - accuracy: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dba7dd0ec8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adjust the Neural Network to the training data\n",
    "neural.fit(X_train, y_train, epochs = 300, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Predictions\n",
    "y_pred = neural.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a 0.5 threshold to distinguis between the two class predictions.\n",
    "y_pred = y_pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Confusion Matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting a fancier confusion matrix\n",
    "fig, ax = plt.subplots(figsize = (2.5, 2.5))\n",
    "ax.matshow(cm, cmap = plt.cm.Reds, alpha = 0.3)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x = j, y = i,\n",
    "               s = cm[i,j])\n",
    "        \n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! With the data fixed, the neural network's bias has been solved. What lies ahead is to optimize the NN's peformance by *playing around* with its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tweek the neural network and add one more layer to it, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Network_2():\n",
    "    #Definimos la red neuronal secuencial\n",
    "    model = Sequential()\n",
    "\n",
    "    #Layer of Neurons\n",
    "    model.add(Dense(6,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"relu\",\n",
    "                    input_dim = X_train.shape[1] #Segundo atributo del método\n",
    "                    #shape, las columnas\n",
    "                   ))\n",
    "    \n",
    "    model.add(Dropout(rate = 0.1))# Turns neurons off and \n",
    "                                    #reduces the overfitting risk\n",
    "\n",
    "    #Layer of Neurons\n",
    "    model.add(Dense(units = 6,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"relu\",\n",
    "                   ))\n",
    "    \n",
    "    model.add(Dropout(rate = 0.1))\n",
    "    \n",
    "    #Layer of Neurons\n",
    "    model.add(Dense(units = 6,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"relu\",\n",
    "                   ))\n",
    "    \n",
    "    model.add(Dropout(rate = 0.1))\n",
    "\n",
    "    #Output Layer\n",
    "    model.add(Dense(units = 1,\n",
    "                   kernel_initializer = \"uniform\",\n",
    "                   activation = \"sigmoid\"))\n",
    "    \n",
    "        #Definimos cómo realizará la red neuronal el aprendizaje\n",
    "    model.compile(optimizer = RMSprop(learning_rate = 0.01),\n",
    "                 loss = \"binary_crossentropy\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_2 = Neural_Network_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55890 samples\n",
      "Epoch 1/300\n",
      "55890/55890 [==============================] - 1s 20us/sample - loss: 0.4712 - accuracy: 0.8050\n",
      "Epoch 2/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4395 - accuracy: 0.8140\n",
      "Epoch 3/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4327 - accuracy: 0.8167\n",
      "Epoch 4/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4314 - accuracy: 0.8178\n",
      "Epoch 5/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4299 - accuracy: 0.8180\n",
      "Epoch 6/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4271 - accuracy: 0.8208\n",
      "Epoch 7/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4235 - accuracy: 0.8231\n",
      "Epoch 8/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4244 - accuracy: 0.8222\n",
      "Epoch 9/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4245 - accuracy: 0.8224\n",
      "Epoch 10/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4221 - accuracy: 0.8234\n",
      "Epoch 11/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4241 - accuracy: 0.8240\n",
      "Epoch 12/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4225 - accuracy: 0.8225\n",
      "Epoch 13/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4224 - accuracy: 0.8234\n",
      "Epoch 14/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4258 - accuracy: 0.8212\n",
      "Epoch 15/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4230 - accuracy: 0.8214\n",
      "Epoch 16/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4228 - accuracy: 0.8235\n",
      "Epoch 17/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4246 - accuracy: 0.8231\n",
      "Epoch 18/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4216 - accuracy: 0.8229\n",
      "Epoch 19/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4242 - accuracy: 0.8230\n",
      "Epoch 20/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4204 - accuracy: 0.8238\n",
      "Epoch 21/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4236 - accuracy: 0.8230\n",
      "Epoch 22/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4237 - accuracy: 0.8236\n",
      "Epoch 23/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4225 - accuracy: 0.8221\n",
      "Epoch 24/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4220 - accuracy: 0.8256\n",
      "Epoch 25/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4238 - accuracy: 0.8232\n",
      "Epoch 26/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4212 - accuracy: 0.8242\n",
      "Epoch 27/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4213 - accuracy: 0.8244\n",
      "Epoch 28/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4228 - accuracy: 0.8230\n",
      "Epoch 29/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4219 - accuracy: 0.8252\n",
      "Epoch 30/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4224 - accuracy: 0.8228\n",
      "Epoch 31/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4214 - accuracy: 0.8255\n",
      "Epoch 32/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4203 - accuracy: 0.8245\n",
      "Epoch 33/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4215 - accuracy: 0.8228\n",
      "Epoch 34/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4204 - accuracy: 0.8249\n",
      "Epoch 35/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4236 - accuracy: 0.8223\n",
      "Epoch 36/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4220 - accuracy: 0.8236\n",
      "Epoch 37/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4229 - accuracy: 0.8230\n",
      "Epoch 38/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4220 - accuracy: 0.8228\n",
      "Epoch 39/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4231 - accuracy: 0.8242\n",
      "Epoch 40/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4210 - accuracy: 0.8251\n",
      "Epoch 41/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8240\n",
      "Epoch 42/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4215 - accuracy: 0.8264\n",
      "Epoch 43/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4199 - accuracy: 0.8249\n",
      "Epoch 44/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4207 - accuracy: 0.8243\n",
      "Epoch 45/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4216 - accuracy: 0.8224\n",
      "Epoch 46/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4224 - accuracy: 0.8215\n",
      "Epoch 47/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4237 - accuracy: 0.8246\n",
      "Epoch 48/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4234 - accuracy: 0.8229\n",
      "Epoch 49/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4229 - accuracy: 0.8228\n",
      "Epoch 50/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4216 - accuracy: 0.8255\n",
      "Epoch 51/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4184 - accuracy: 0.8266\n",
      "Epoch 52/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4240 - accuracy: 0.8230\n",
      "Epoch 53/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4229 - accuracy: 0.8230\n",
      "Epoch 54/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4187 - accuracy: 0.8248\n",
      "Epoch 55/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4176 - accuracy: 0.8259\n",
      "Epoch 56/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4211 - accuracy: 0.8234\n",
      "Epoch 57/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4231 - accuracy: 0.8235\n",
      "Epoch 58/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4223 - accuracy: 0.8230\n",
      "Epoch 59/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4217 - accuracy: 0.8246\n",
      "Epoch 60/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4219 - accuracy: 0.8244\n",
      "Epoch 61/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4229 - accuracy: 0.8248\n",
      "Epoch 62/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4222 - accuracy: 0.8246\n",
      "Epoch 63/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8251\n",
      "Epoch 64/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4201 - accuracy: 0.8248\n",
      "Epoch 65/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4231 - accuracy: 0.8240\n",
      "Epoch 66/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4220 - accuracy: 0.8240\n",
      "Epoch 67/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4220 - accuracy: 0.8233\n",
      "Epoch 68/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4190 - accuracy: 0.8249\n",
      "Epoch 69/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4225 - accuracy: 0.8239\n",
      "Epoch 70/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4208 - accuracy: 0.8254\n",
      "Epoch 71/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4218 - accuracy: 0.8243\n",
      "Epoch 72/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4212 - accuracy: 0.8246\n",
      "Epoch 73/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4212 - accuracy: 0.8254\n",
      "Epoch 74/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4242 - accuracy: 0.8241\n",
      "Epoch 75/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4217 - accuracy: 0.8252\n",
      "Epoch 76/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4210 - accuracy: 0.8232\n",
      "Epoch 77/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4220 - accuracy: 0.8249\n",
      "Epoch 78/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4245 - accuracy: 0.8253\n",
      "Epoch 79/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4218 - accuracy: 0.8253\n",
      "Epoch 80/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4246 - accuracy: 0.8233\n",
      "Epoch 81/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4217 - accuracy: 0.8237\n",
      "Epoch 82/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4223 - accuracy: 0.8259\n",
      "Epoch 83/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4203 - accuracy: 0.8258\n",
      "Epoch 84/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4221 - accuracy: 0.8235\n",
      "Epoch 85/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4211 - accuracy: 0.8246\n",
      "Epoch 86/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4186 - accuracy: 0.8260\n",
      "Epoch 87/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4220 - accuracy: 0.8241\n",
      "Epoch 88/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8260\n",
      "Epoch 89/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4224 - accuracy: 0.8240\n",
      "Epoch 90/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4223 - accuracy: 0.8245\n",
      "Epoch 91/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4252 - accuracy: 0.8246\n",
      "Epoch 92/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4231 - accuracy: 0.8247\n",
      "Epoch 93/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4261 - accuracy: 0.8213\n",
      "Epoch 94/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4211 - accuracy: 0.8247\n",
      "Epoch 95/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4211 - accuracy: 0.8257\n",
      "Epoch 96/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4212 - accuracy: 0.8249\n",
      "Epoch 97/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4206 - accuracy: 0.8259\n",
      "Epoch 98/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4223 - accuracy: 0.8245\n",
      "Epoch 99/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4207 - accuracy: 0.8277\n",
      "Epoch 100/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4206 - accuracy: 0.8248\n",
      "Epoch 101/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4224 - accuracy: 0.8247\n",
      "Epoch 102/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4224 - accuracy: 0.8227\n",
      "Epoch 103/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4221 - accuracy: 0.8241\n",
      "Epoch 104/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4220 - accuracy: 0.8241\n",
      "Epoch 105/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4197 - accuracy: 0.8247\n",
      "Epoch 106/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4233 - accuracy: 0.8240\n",
      "Epoch 107/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4229 - accuracy: 0.8230\n",
      "Epoch 108/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4222 - accuracy: 0.8259\n",
      "Epoch 109/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4241 - accuracy: 0.8237\n",
      "Epoch 110/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4197 - accuracy: 0.8267\n",
      "Epoch 111/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4221 - accuracy: 0.8246\n",
      "Epoch 112/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4211 - accuracy: 0.8259\n",
      "Epoch 113/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4237 - accuracy: 0.8251\n",
      "Epoch 114/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4207 - accuracy: 0.8266\n",
      "Epoch 115/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8245\n",
      "Epoch 116/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4224 - accuracy: 0.8242\n",
      "Epoch 117/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4226 - accuracy: 0.8247\n",
      "Epoch 118/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4219 - accuracy: 0.8242\n",
      "Epoch 119/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4214 - accuracy: 0.8260\n",
      "Epoch 120/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4230 - accuracy: 0.8230\n",
      "Epoch 121/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4210 - accuracy: 0.8256\n",
      "Epoch 122/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4184 - accuracy: 0.8254\n",
      "Epoch 123/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4194 - accuracy: 0.8251\n",
      "Epoch 124/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4219 - accuracy: 0.8249\n",
      "Epoch 125/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8241\n",
      "Epoch 126/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4231 - accuracy: 0.8238\n",
      "Epoch 127/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4217 - accuracy: 0.8237\n",
      "Epoch 128/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4227 - accuracy: 0.8250\n",
      "Epoch 129/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4230 - accuracy: 0.8250\n",
      "Epoch 130/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4219 - accuracy: 0.8244\n",
      "Epoch 131/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4213 - accuracy: 0.8256\n",
      "Epoch 132/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4235 - accuracy: 0.8236\n",
      "Epoch 133/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4207 - accuracy: 0.8250\n",
      "Epoch 134/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4226 - accuracy: 0.8240\n",
      "Epoch 135/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4226 - accuracy: 0.8241\n",
      "Epoch 136/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4235 - accuracy: 0.8256\n",
      "Epoch 137/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4206 - accuracy: 0.8260\n",
      "Epoch 138/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4230 - accuracy: 0.8261\n",
      "Epoch 139/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4185 - accuracy: 0.8269\n",
      "Epoch 140/300\n",
      "55890/55890 [==============================] - 0s 9us/sample - loss: 0.4201 - accuracy: 0.8248\n",
      "Epoch 141/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4229 - accuracy: 0.8227\n",
      "Epoch 142/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4209 - accuracy: 0.8251\n",
      "Epoch 143/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4223 - accuracy: 0.8262\n",
      "Epoch 144/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4213 - accuracy: 0.8244\n",
      "Epoch 145/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4215 - accuracy: 0.8247\n",
      "Epoch 146/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4215 - accuracy: 0.8249\n",
      "Epoch 147/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4217 - accuracy: 0.8235\n",
      "Epoch 148/300\n",
      "55890/55890 [==============================] - 0s 8us/sample - loss: 0.4212 - accuracy: 0.8251\n",
      "Epoch 149/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4233 - accuracy: 0.8251\n",
      "Epoch 150/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4237 - accuracy: 0.8223\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4237 - accuracy: 0.8234\n",
      "Epoch 152/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4221 - accuracy: 0.8252\n",
      "Epoch 153/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4240 - accuracy: 0.8232\n",
      "Epoch 154/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4221 - accuracy: 0.8247\n",
      "Epoch 155/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4225 - accuracy: 0.8246\n",
      "Epoch 156/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4188 - accuracy: 0.8263\n",
      "Epoch 157/300\n",
      "55890/55890 [==============================] - 1s 9us/sample - loss: 0.4230 - accuracy: 0.8240\n",
      "Epoch 158/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4212 - accuracy: 0.8250\n",
      "Epoch 159/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4219 - accuracy: 0.8250\n",
      "Epoch 160/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4223 - accuracy: 0.8256\n",
      "Epoch 161/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4240 - accuracy: 0.8247\n",
      "Epoch 162/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4222 - accuracy: 0.8246\n",
      "Epoch 163/300\n",
      "55890/55890 [==============================] - 1s 10us/sample - loss: 0.4243 - accuracy: 0.8238\n",
      "Epoch 164/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4202 - accuracy: 0.8259\n",
      "Epoch 165/300\n",
      "55890/55890 [==============================] - 1s 16us/sample - loss: 0.4216 - accuracy: 0.8216\n",
      "Epoch 166/300\n",
      "55890/55890 [==============================] - 1s 16us/sample - loss: 0.4229 - accuracy: 0.8260\n",
      "Epoch 167/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4239 - accuracy: 0.8253\n",
      "Epoch 168/300\n",
      "55890/55890 [==============================] - 1s 26us/sample - loss: 0.4216 - accuracy: 0.8240\n",
      "Epoch 169/300\n",
      "55890/55890 [==============================] - 1s 18us/sample - loss: 0.4237 - accuracy: 0.8251\n",
      "Epoch 170/300\n",
      "55890/55890 [==============================] - 1s 20us/sample - loss: 0.4239 - accuracy: 0.8241\n",
      "Epoch 171/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4215 - accuracy: 0.8245\n",
      "Epoch 172/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4184 - accuracy: 0.8251\n",
      "Epoch 173/300\n",
      "55890/55890 [==============================] - 1s 17us/sample - loss: 0.4226 - accuracy: 0.8235\n",
      "Epoch 174/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4222 - accuracy: 0.8248\n",
      "Epoch 175/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4219 - accuracy: 0.8246\n",
      "Epoch 176/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4233 - accuracy: 0.8247\n",
      "Epoch 177/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4217 - accuracy: 0.8255\n",
      "Epoch 178/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4207 - accuracy: 0.8249\n",
      "Epoch 179/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4232 - accuracy: 0.8231\n",
      "Epoch 180/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4224 - accuracy: 0.8234\n",
      "Epoch 181/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4245 - accuracy: 0.8223\n",
      "Epoch 182/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4219 - accuracy: 0.8231\n",
      "Epoch 183/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4194 - accuracy: 0.8246\n",
      "Epoch 184/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4232 - accuracy: 0.8247\n",
      "Epoch 185/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4225 - accuracy: 0.8263\n",
      "Epoch 186/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4234 - accuracy: 0.8234\n",
      "Epoch 187/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4216 - accuracy: 0.8244\n",
      "Epoch 188/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4221 - accuracy: 0.8245\n",
      "Epoch 189/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4222 - accuracy: 0.8252\n",
      "Epoch 190/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4234 - accuracy: 0.8244\n",
      "Epoch 191/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4203 - accuracy: 0.8267\n",
      "Epoch 192/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4231 - accuracy: 0.8231\n",
      "Epoch 193/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4222 - accuracy: 0.8249\n",
      "Epoch 194/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4225 - accuracy: 0.8237\n",
      "Epoch 195/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4225 - accuracy: 0.8246\n",
      "Epoch 196/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4220 - accuracy: 0.8275\n",
      "Epoch 197/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4211 - accuracy: 0.8237\n",
      "Epoch 198/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4228 - accuracy: 0.8237\n",
      "Epoch 199/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4221 - accuracy: 0.8250\n",
      "Epoch 200/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4225 - accuracy: 0.8237\n",
      "Epoch 201/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4240 - accuracy: 0.8244s - loss: 0.4247 - accu\n",
      "Epoch 202/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4221 - accuracy: 0.8241\n",
      "Epoch 203/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4245 - accuracy: 0.8219\n",
      "Epoch 204/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4222 - accuracy: 0.8244\n",
      "Epoch 205/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4230 - accuracy: 0.8226\n",
      "Epoch 206/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4221 - accuracy: 0.8246\n",
      "Epoch 207/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4220 - accuracy: 0.8227\n",
      "Epoch 208/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4246 - accuracy: 0.8253\n",
      "Epoch 209/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4218 - accuracy: 0.8231\n",
      "Epoch 210/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4221 - accuracy: 0.8247\n",
      "Epoch 211/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4213 - accuracy: 0.8254\n",
      "Epoch 212/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4237 - accuracy: 0.8239\n",
      "Epoch 213/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4206 - accuracy: 0.8256\n",
      "Epoch 214/300\n",
      "55890/55890 [==============================] - 1s 16us/sample - loss: 0.4213 - accuracy: 0.8257\n",
      "Epoch 215/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4233 - accuracy: 0.8232\n",
      "Epoch 216/300\n",
      "55890/55890 [==============================] - 1s 18us/sample - loss: 0.4229 - accuracy: 0.8245\n",
      "Epoch 217/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4221 - accuracy: 0.8233\n",
      "Epoch 218/300\n",
      "55890/55890 [==============================] - 1s 17us/sample - loss: 0.4219 - accuracy: 0.8256\n",
      "Epoch 219/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4207 - accuracy: 0.8246\n",
      "Epoch 220/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4210 - accuracy: 0.8250\n",
      "Epoch 221/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4236 - accuracy: 0.8254\n",
      "Epoch 222/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4213 - accuracy: 0.8252\n",
      "Epoch 223/300\n",
      "55890/55890 [==============================] - 1s 18us/sample - loss: 0.4235 - accuracy: 0.8249\n",
      "Epoch 224/300\n",
      "55890/55890 [==============================] - 1s 16us/sample - loss: 0.4225 - accuracy: 0.8228\n",
      "Epoch 225/300\n",
      "55890/55890 [==============================] - 1s 18us/sample - loss: 0.4244 - accuracy: 0.8243\n",
      "Epoch 226/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4216 - accuracy: 0.8249\n",
      "Epoch 227/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4219 - accuracy: 0.8245\n",
      "Epoch 228/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4204 - accuracy: 0.8236\n",
      "Epoch 229/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4215 - accuracy: 0.8265\n",
      "Epoch 230/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4216 - accuracy: 0.8244\n",
      "Epoch 231/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4226 - accuracy: 0.8254\n",
      "Epoch 232/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4211 - accuracy: 0.8259\n",
      "Epoch 233/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4233 - accuracy: 0.8235\n",
      "Epoch 234/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4222 - accuracy: 0.8250\n",
      "Epoch 235/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4213 - accuracy: 0.8249\n",
      "Epoch 236/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4207 - accuracy: 0.8244\n",
      "Epoch 237/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4220 - accuracy: 0.8252\n",
      "Epoch 238/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4207 - accuracy: 0.8262\n",
      "Epoch 239/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4190 - accuracy: 0.8261\n",
      "Epoch 240/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4203 - accuracy: 0.8257\n",
      "Epoch 241/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4223 - accuracy: 0.8252\n",
      "Epoch 242/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4205 - accuracy: 0.8261\n",
      "Epoch 243/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4206 - accuracy: 0.8256\n",
      "Epoch 244/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4219 - accuracy: 0.8248\n",
      "Epoch 245/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4235 - accuracy: 0.8235\n",
      "Epoch 246/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4224 - accuracy: 0.8247\n",
      "Epoch 247/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4201 - accuracy: 0.8249\n",
      "Epoch 248/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4221 - accuracy: 0.8245\n",
      "Epoch 249/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4202 - accuracy: 0.8258\n",
      "Epoch 250/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4229 - accuracy: 0.8252\n",
      "Epoch 251/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4237 - accuracy: 0.8232\n",
      "Epoch 252/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4216 - accuracy: 0.8238\n",
      "Epoch 253/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4229 - accuracy: 0.8232\n",
      "Epoch 254/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4235 - accuracy: 0.8218\n",
      "Epoch 255/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4204 - accuracy: 0.8251\n",
      "Epoch 256/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4206 - accuracy: 0.8270\n",
      "Epoch 257/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4222 - accuracy: 0.8245\n",
      "Epoch 258/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4202 - accuracy: 0.8247\n",
      "Epoch 259/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4203 - accuracy: 0.8237\n",
      "Epoch 260/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4211 - accuracy: 0.8274\n",
      "Epoch 261/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4243 - accuracy: 0.8230\n",
      "Epoch 262/300\n",
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4224 - accuracy: 0.8242\n",
      "Epoch 263/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4214 - accuracy: 0.8250\n",
      "Epoch 264/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4223 - accuracy: 0.8241\n",
      "Epoch 265/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4209 - accuracy: 0.8238\n",
      "Epoch 266/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4214 - accuracy: 0.8248\n",
      "Epoch 267/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4215 - accuracy: 0.8236\n",
      "Epoch 268/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4220 - accuracy: 0.8249\n",
      "Epoch 269/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4222 - accuracy: 0.8243\n",
      "Epoch 270/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4225 - accuracy: 0.8252\n",
      "Epoch 271/300\n",
      "55890/55890 [==============================] - 1s 16us/sample - loss: 0.4226 - accuracy: 0.8253\n",
      "Epoch 272/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4232 - accuracy: 0.8239\n",
      "Epoch 273/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4220 - accuracy: 0.8249\n",
      "Epoch 274/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4245 - accuracy: 0.8247\n",
      "Epoch 275/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4219 - accuracy: 0.8247\n",
      "Epoch 276/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4216 - accuracy: 0.8237\n",
      "Epoch 277/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4224 - accuracy: 0.8241s - loss: 0.4189 - accu\n",
      "Epoch 278/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4204 - accuracy: 0.8252\n",
      "Epoch 279/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4229 - accuracy: 0.8236\n",
      "Epoch 280/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4220 - accuracy: 0.8252\n",
      "Epoch 281/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4221 - accuracy: 0.8249\n",
      "Epoch 282/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4208 - accuracy: 0.8249\n",
      "Epoch 283/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4241 - accuracy: 0.8232\n",
      "Epoch 284/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4231 - accuracy: 0.8242\n",
      "Epoch 285/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4226 - accuracy: 0.8236\n",
      "Epoch 286/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4216 - accuracy: 0.8243\n",
      "Epoch 287/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4220 - accuracy: 0.8239\n",
      "Epoch 288/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4249 - accuracy: 0.8246\n",
      "Epoch 289/300\n",
      "55890/55890 [==============================] - 1s 15us/sample - loss: 0.4233 - accuracy: 0.8239\n",
      "Epoch 290/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4230 - accuracy: 0.8248\n",
      "Epoch 291/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4210 - accuracy: 0.8246\n",
      "Epoch 292/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4210 - accuracy: 0.8243\n",
      "Epoch 293/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4210 - accuracy: 0.8269\n",
      "Epoch 294/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4239 - accuracy: 0.8229\n",
      "Epoch 295/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4240 - accuracy: 0.8241\n",
      "Epoch 296/300\n",
      "55890/55890 [==============================] - 1s 12us/sample - loss: 0.4199 - accuracy: 0.8260\n",
      "Epoch 297/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4258 - accuracy: 0.8235\n",
      "Epoch 298/300\n",
      "55890/55890 [==============================] - 1s 14us/sample - loss: 0.4209 - accuracy: 0.8247\n",
      "Epoch 299/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55890/55890 [==============================] - 1s 11us/sample - loss: 0.4219 - accuracy: 0.8238\n",
      "Epoch 300/300\n",
      "55890/55890 [==============================] - 1s 13us/sample - loss: 0.4218 - accuracy: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbaaf62a88>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_2.fit(X_train, y_train, epochs = 300, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_2.predict(X_test)\n",
    "y_pred = y_pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "#Plotting a fancier confusion matrix\n",
    "fig, ax = plt.subplots(figsize = (2.5, 2.5))\n",
    "ax.matshow(cm, cmap = plt.cm.Greens, alpha = 0.3)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x = j, y = i,\n",
    "               s = cm[i,j])\n",
    "        \n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23954/23954 [==============================] - 0s 20us/sample - loss: 0.3728 - accuracy: 0.8632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.372764693801885, 0.86323786]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23954/23954 [==============================] - 0s 20us/sample - loss: 0.3088 - accuracy: 0.8724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30876632127467624, 0.8723804]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NN did not perform that well this time around. The extra layer is not really worth it, so I will optimize the **Neural_Network()** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Hyperparameter's Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the neural network I am going to use the *GidSearchCV* function from *sklearn*, which will be provided with a dictionary containing the several parameters to optimize with a pair of values to iterate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the neural network once more, leaving this time some parameters to be defined by the dictionary that will be passed to the *GridSearchCV* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural(optimizer, learning_rate):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, \n",
    "                         kernel_initializer = \"uniform\",\n",
    "                         activation = \"relu\",\n",
    "                         input_dim = X_train.shape[1]))\n",
    "    \n",
    "    classifier.add(Dense(units = 6, kernel_initializer = \"uniform\", \n",
    "                         activation = \"relu\"))\n",
    "    \n",
    "    classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  \n",
    "                         activation = \"sigmoid\"))\n",
    "    \n",
    "    classifier.compile(optimizer = optimizer(learning_rate),\n",
    "                       loss = \"binary_crossentropy\", \n",
    "                       metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = Neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary with the parameters to optimize and the values to try out.\n",
    "parameters = {\n",
    "    'batch_size' : [25, 32],\n",
    "    'nb_epoch' : [100, 300],\n",
    "    'optimizer': [Adam, RMSprop],\n",
    "    'learning_rate': [0.01, 0.03]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy', \n",
    "                           cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3711 - accuracy: 0.8392\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3720 - accuracy: 0.8389\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 43us/sample - loss: 0.3789 - accuracy: 0.8386\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3702 - accuracy: 0.8403\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3693 - accuracy: 0.8403\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.3777 - accuracy: 0.8373\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3695 - accuracy: 0.8380\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 55us/sample - loss: 0.3751 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3708 - accuracy: 0.8395\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3697 - accuracy: 0.8424\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3774 - accuracy: 0.8363\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 55us/sample - loss: 0.3737 - accuracy: 0.8370\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 61us/sample - loss: 0.3742 - accuracy: 0.8402\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 53us/sample - loss: 0.3724 - accuracy: 0.8415\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 52us/sample - loss: 0.3716 - accuracy: 0.8396\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 49us/sample - loss: 0.3787 - accuracy: 0.8376\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 50us/sample - loss: 0.3952 - accuracy: 0.8388\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3725 - accuracy: 0.8415\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3785 - accuracy: 0.8379\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 50us/sample - loss: 0.3965 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3720 - accuracy: 0.8388\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3749 - accuracy: 0.8361\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 49us/sample - loss: 0.3688 - accuracy: 0.8411\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3725 - accuracy: 0.8390\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.4028 - accuracy: 0.8286\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3681 - accuracy: 0.8427\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3782 - accuracy: 0.8377\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3935 - accuracy: 0.8348\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3843 - accuracy: 0.8341\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3769 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3767 - accuracy: 0.8382\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 62us/sample - loss: 0.3750 - accuracy: 0.8384\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 51us/sample - loss: 0.3785 - accuracy: 0.8381\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 50us/sample - loss: 0.3724 - accuracy: 0.8422\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 53us/sample - loss: 0.3727 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 59us/sample - loss: 0.3785 - accuracy: 0.8383\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 50us/sample - loss: 0.3741 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 65us/sample - loss: 0.3725 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 4s 70us/sample - loss: 0.3861 - accuracy: 0.8347\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 52us/sample - loss: 0.3983 - accuracy: 0.8352\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 58us/sample - loss: 0.4000 - accuracy: 0.8325\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3768 - accuracy: 0.8379\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3772 - accuracy: 0.8372\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 4s 77us/sample - loss: 0.3756 - accuracy: 0.8389\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3815 - accuracy: 0.8358\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 50us/sample - loss: 0.3721 - accuracy: 0.8421\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.3730 - accuracy: 0.8402\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 51us/sample - loss: 0.3991 - accuracy: 0.8315\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.3799 - accuracy: 0.8362\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.4258 - accuracy: 0.8213\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3835 - accuracy: 0.8369\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3768 - accuracy: 0.8417\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3779 - accuracy: 0.8402\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3782 - accuracy: 0.8400\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3801 - accuracy: 0.8417\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3791 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3754 - accuracy: 0.8417\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 55us/sample - loss: 0.3804 - accuracy: 0.8403\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 56us/sample - loss: 0.3822 - accuracy: 0.8365\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 4s 72us/sample - loss: 0.3770 - accuracy: 0.8403\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.3765 - accuracy: 0.8382\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 57us/sample - loss: 0.3785 - accuracy: 0.8382\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3790 - accuracy: 0.8360\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3726 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3782 - accuracy: 0.8387\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 51us/sample - loss: 0.3755 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3743 - accuracy: 0.8390\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 49us/sample - loss: 0.3969 - accuracy: 0.8334\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 44us/sample - loss: 0.3756 - accuracy: 0.8383\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3781 - accuracy: 0.8382\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 53us/sample - loss: 0.3818 - accuracy: 0.8375\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3806 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 50us/sample - loss: 0.3790 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 55us/sample - loss: 0.4020 - accuracy: 0.8332\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 67us/sample - loss: 0.3816 - accuracy: 0.8387\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 61us/sample - loss: 0.3765 - accuracy: 0.8400\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 59us/sample - loss: 0.3867 - accuracy: 0.8388\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 63us/sample - loss: 0.3820 - accuracy: 0.8379\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 66us/sample - loss: 0.3779 - accuracy: 0.8432\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 58us/sample - loss: 0.3797 - accuracy: 0.8392\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 53us/sample - loss: 0.3782 - accuracy: 0.8330\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3738 - accuracy: 0.8388\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 44us/sample - loss: 0.3727 - accuracy: 0.8381\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3688 - accuracy: 0.8416\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 65us/sample - loss: 0.3720 - accuracy: 0.8389\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3731 - accuracy: 0.8378\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 37us/sample - loss: 0.3722 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.4067 - accuracy: 0.8295\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3825 - accuracy: 0.8337\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 37us/sample - loss: 0.3731 - accuracy: 0.8354\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3749 - accuracy: 0.8390\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.4164 - accuracy: 0.8270\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3730 - accuracy: 0.8376\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3713 - accuracy: 0.8418\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 48us/sample - loss: 0.3731 - accuracy: 0.8410\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 58us/sample - loss: 0.3732 - accuracy: 0.8389\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3740 - accuracy: 0.8372\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 52us/sample - loss: 0.3724 - accuracy: 0.8404\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 52us/sample - loss: 0.3778 - accuracy: 0.8361\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3794 - accuracy: 0.8341\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 40us/sample - loss: 0.3990 - accuracy: 0.8315\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3715 - accuracy: 0.8389\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 50us/sample - loss: 0.3733 - accuracy: 0.8379\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 44us/sample - loss: 0.4048 - accuracy: 0.8295\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.83 - 2s 47us/sample - loss: 0.3728 - accuracy: 0.8355\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.4043 - accuracy: 0.8318\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 40us/sample - loss: 0.3702 - accuracy: 0.8405\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 44us/sample - loss: 0.3737 - accuracy: 0.8351\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 57us/sample - loss: 0.3826 - accuracy: 0.8351\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 51us/sample - loss: 0.4012 - accuracy: 0.8333\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 64us/sample - loss: 0.3754 - accuracy: 0.8391\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 69us/sample - loss: 0.3734 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 52us/sample - loss: 0.4254 - accuracy: 0.8256\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3843 - accuracy: 0.8368\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 66us/sample - loss: 0.3737 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 56us/sample - loss: 0.3991 - accuracy: 0.8366\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 45us/sample - loss: 0.3698 - accuracy: 0.8418\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 54us/sample - loss: 0.3710 - accuracy: 0.8413\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3804 - accuracy: 0.8373\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 40us/sample - loss: 0.3767 - accuracy: 0.8394\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 63us/sample - loss: 0.3834 - accuracy: 0.8341\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 53us/sample - loss: 0.3795 - accuracy: 0.8367\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3742 - accuracy: 0.8385\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3785 - accuracy: 0.8379\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3743 - accuracy: 0.8408\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3748 - accuracy: 0.8380\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3732 - accuracy: 0.8385\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3790 - accuracy: 0.8377\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 37us/sample - loss: 0.3785 - accuracy: 0.8372\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3791 - accuracy: 0.8368\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3781 - accuracy: 0.8393\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3761 - accuracy: 0.8426\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3783 - accuracy: 0.8404\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3770 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3767 - accuracy: 0.8423\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3875 - accuracy: 0.8374\n",
      "Train on 50301 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3760 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3765 - accuracy: 0.8385\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3781 - accuracy: 0.8409\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 46us/sample - loss: 0.3742 - accuracy: 0.8429\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3748 - accuracy: 0.8374\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.4046 - accuracy: 0.8267\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3766 - accuracy: 0.8388\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.3771 - accuracy: 0.8377\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3744 - accuracy: 0.8401\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 42us/sample - loss: 0.3736 - accuracy: 0.8397\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 35us/sample - loss: 0.3772 - accuracy: 0.8369\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 36us/sample - loss: 0.4212 - accuracy: 0.8236\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 41us/sample - loss: 0.3918 - accuracy: 0.8323\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 37us/sample - loss: 0.3734 - accuracy: 0.8385\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3779 - accuracy: 0.8380\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.4255 - accuracy: 0.8221\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3819 - accuracy: 0.8384\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 38us/sample - loss: 0.3803 - accuracy: 0.8361\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3805 - accuracy: 0.8387\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 3s 57us/sample - loss: 0.4016 - accuracy: 0.8313\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 47us/sample - loss: 0.3759 - accuracy: 0.8422\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.3759 - accuracy: 0.8409\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 39us/sample - loss: 0.4099 - accuracy: 0.8262\n",
      "Train on 50301 samples\n",
      "50301/50301 [==============================] - 2s 37us/sample - loss: 0.3761 - accuracy: 0.8377\n",
      "Train on 55890 samples\n",
      "55890/55890 [==============================] - 2s 37us/sample - loss: 0.3781 - accuracy: 0.8399\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'batch_size': 32, 'learning_rate': 0.01, 'nb_epoch': 300, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop'>}\n",
      "Best Score of the Neural Network:  0.8518876364286992\n"
     ]
    }
   ],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print('The best parameters are: ', best_parameters)\n",
    "print('Best Score of the Neural Network: ', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network managed to obtain good results in classifying the two classes after upsampling the minority class, which is also required for a \"simpler\" algorithm, and training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook I have built a simple Two-Class Neural Network and got it, by optimizing the parameters, to an 86% accuracy. Nevertheless, in the notebook *Imbalanced Data* the *Random Forest* algorithm worked better, with the scores:\n",
    "\n",
    "- Accuracy: 96.6%\n",
    "- Recall: 99.75%\n",
    "- f1 Score: 96.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the *Random Forest* outperformed the Neural Network by 10% in *accuracy*. So, to train a Neural Network might not be worth it for every problem due to:\n",
    "\n",
    "- A neural network requires lots of data to be properly trained.\n",
    "- NNs require long training time to get to highly accurate predictions.\n",
    "\n",
    "\n",
    "- Simpler algorithms such as a Random Forest can outperform them with much less training samples and time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
